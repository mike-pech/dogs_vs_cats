{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (4.35.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: torch in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\python311\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\python311\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: click in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\python311\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\danil\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers nltk scikit-learn numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "import tqdm as notebook_tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./IMDB Dataset.csv\")\n",
    "data.replace(to_replace={\"sentiment\": {\"negative\": 0, \"positive\": 1}}, inplace=True)\n",
    "data[\"review\"] = data[\"review\"].apply(lambda r: r.replace(\"<br />\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_quoted = re.compile(r\"\\\".*?\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем версию без наименований в ковычках. Их исключение повышает точность моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names.words()\n",
    "cleanedup_data = data.copy(deep=True)\n",
    "cleanedup_data[\"review\"] = cleanedup_data[\"review\"].apply(\n",
    "    lambda r: re.sub(rm_quoted, \"\", r.replace(\"<br />\", \"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также лемматизируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrStopwords = stopwords.words(\"english\")\n",
    "arrStopwords.extend([\"br\", \"<br>\", \"<br />\"])\n",
    "lemm_arr = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for r in data.iterrows():\n",
    "    rev = r[1][\"review\"]\n",
    "\n",
    "    rev_no_quotes = re.sub(rm_quoted, \"\", rev)\n",
    "    rev_tok = word_tokenize(rev_no_quotes)\n",
    "    rev_tok = [w.lower() for w in rev_tok]\n",
    "\n",
    "    rev_tok_filtered = [w for w in rev_tok if not w in arrStopwords]\n",
    "    rev_tok_filtered = [w for w in rev_tok_filtered if w.isalpha()]\n",
    "\n",
    "    rev_tok_lemmatized = np.array([wnl.lemmatize(w) for w in rev_tok_filtered])\n",
    "\n",
    "    # reviews_tok_arr.append(rev_tok_filtered)\n",
    "    lemm_arr.append((rev_tok_lemmatized, r[1][\"sentiment\"]))\n",
    "\n",
    "lemm_data = pd.DataFrame(columns=[\"reviews\", \"sentiment\"], data=lemm_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_results = {\"raw\" : data, \"cleaned\" : cleanedup_data, \"lemmatized\" : lemm_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analyzer встроенный в NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В NLTK есть предобученный определитель настроения называемый VADER. Можно решить задачу при помощи него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\danil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 69.76%\n"
     ]
    }
   ],
   "source": [
    "vader_predictions = []\n",
    "\n",
    "for r in cleanedup_data.iterrows():\n",
    "    rev = r[1][\"review\"]\n",
    "\n",
    "    vader_predictions.append(\n",
    "        0 if vader.polarity_scores(rev)[\"compound\"] < 0 else 1\n",
    "    )\n",
    "\n",
    "print(f\"Точность: {accuracy_score(cleanedup_data['sentiment'], vader_predictions) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже так можно добится точности в районе 70%. На несколько процентов точность можно повысить, если добавить дополнительные признаки и использовать классификаторы из scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала получим списоки отрицательных и положительных слов из корпуса NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_en = nltk.corpus.stopwords.words(\"english\")\n",
    "stopword_en.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in stopword_en:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом выберем самые часто встречающиеся положительные и отрицательные леммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive_lem = {wnl.lemmatize(w) for w, count in positive_fd.most_common(100)}\n",
    "top_100_negative_lem = {wnl.lemmatize(w) for w, count in negative_fd.most_common(100)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим классифаеры из scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для экстракции признаков. Она собирает количество негативных и позитивных слов, а также значения выдаваемые VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    pos_word_count = 0\n",
    "    neg_word_count = 0\n",
    "\n",
    "    for r in wnl.lemmatize(text):\n",
    "        for w in r:\n",
    "            if w in top_100_negative_lem:\n",
    "                neg_word_count += 1\n",
    "            elif w in top_100_positive_lem:\n",
    "                pos_word_count += 1\n",
    "\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "    \n",
    "    features[\"compound\"] = vader_scores[\"compound\"] + 1\n",
    "    features[\"neg\"] = vader_scores[\"neg\"]\n",
    "    features[\"pos\"] = vader_scores[\"pos\"]\n",
    "    features[\"pos_word_count\"] = pos_word_count\n",
    "    features[\"neg_word_count\"] = neg_word_count\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собирём признаки для всего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurelist = []\n",
    "feature_names = list(extract_features(\"I like pickles\"))\n",
    "data_column_names = feature_names + [\"sentiment\"]\n",
    "\n",
    "for r in cleanedup_data.iterrows():\n",
    "    r_features_dict = extract_features(r[1][\"review\"])\n",
    "    r_features_dict[\"sentiment\"] = r[1][\"sentiment\"]\n",
    "    featurelist.append(r_features_dict)\n",
    "\n",
    "features_df = pd.DataFrame(data=featurelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем признаки как вводный массив для разных моделей sklearn. Как видим точность некоторых моделей привосходит точность VADER самого по себе. Теоретически можно попробовать вывести ещё больше значимых признаков из текстов данного датасета и увеличить тем самым точность ещё выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.15% - BernoulliNB\n",
      "70.08% - ComplementNB\n",
      "69.99% - MultinomialNB\n",
      "69.39% - KNeighborsClassifier\n",
      "64.37% - DecisionTreeClassifier\n",
      "70.64% - RandomForestClassifier\n",
      "72.76% - MLPClassifier\n",
      "73.07% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(features_df, train_size=0.85)\n",
    "\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    cls = sklearn_classifier\n",
    "\n",
    "    x_train = train_data.drop(columns=[\"sentiment\"])\n",
    "    y_train = train_data[\"sentiment\"]\n",
    "\n",
    "    x_test = test_data.drop(columns=[\"sentiment\"])\n",
    "    y_test = test_data[\"sentiment\"]\n",
    "\n",
    "    model = cls.fit(X=x_train, y=y_train)\n",
    "\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(F\"{accuracy:.2%} - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главным приемуществом данного метода является его скорость. Обработать весь датасет при помощи VADER можно за секунды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 30s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 1min 30s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit -n1 -r1 -o\n",
    "dummy_predictions = []\n",
    "for r in cleanedup_data.iterrows():\n",
    "    rev = r[1][\"review\"]\n",
    "\n",
    "    dummy_predictions.append(\n",
    "        0 if vader.polarity_scores(rev)[\"compound\"] < 0 else 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_worktime = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### **Главным приемуществом данного метода является его скорость. Обработать весь датасет при помощи VADER можно за 91.00 секунды**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"### **Главным приемуществом данного метода является его скорость. Обработать весь датасет при помощи VADER можно за {vader_worktime.average:.2f} секунды**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели с Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также для анализа можно использовать модели предтренированные работоприятные с Hugging Face. Например [DistilBERT base uncased finetuned SST-2](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). Главная проблема заключается в том, что глубинные нейронные сети почти в 100 раз медленней алгоритма VADER и на анализ всего датасета уйдет более 7 часов. Так что для демонстрации и предварительного вычисления точности мы используем небольшое подмножество датасета из 1000 отзывов. Данная модель также не может обрабатывать более 512 токенов, так что нам придётся обрезать каждый отзыв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 ms ± 5.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "model_processing_time = %timeit sentiment_pipeline(data[\"review\"][randint(1, 50000)][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 ms ± 83.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "vader_processing_time = %timeit vader.polarity_scores(data[\"review\"][randint(1, 50000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_test_dataset = shuffle(data, random_state=42).iloc[:1000].copy(deep=True)\n",
    "ml_test_dataset[\"review\"] = ml_test_dataset[\"review\"].apply(lambda r: r[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([\n",
    "    0 if p[\"label\"] == \"NEGATIVE\" else 1\n",
    "    for p in sentiment_pipeline(list(ml_test_dataset[\"review\"]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.70% - DistilBERT\n"
     ]
    }
   ],
   "source": [
    "ml_accuracy = accuracy_score(ml_test_dataset[\"sentiment\"], predictions)\n",
    "print(f\"{ml_accuracy * 100:.2f}% - DistilBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тем не менее DistilBERT дает точность предсказаний гораздо выше других моделей - в районе 80%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
